{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "87200a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "215bcfbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\llaur\\\\OneDrive\\\\Desktop\\\\turb\\\\proj-turbulence\\\\test'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fe796c40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\llaur\\Downloads\\ERA_wind\n"
     ]
    }
   ],
   "source": [
    "%cd ../../../../../Downloads/ERA_wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "24b1d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine = np.load(\"ERA_wind.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c06d039",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc9a641",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ea169609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\llaur\\OneDrive\\Desktop\\turb\\proj-turbulence\\train\n"
     ]
    }
   ],
   "source": [
    "%cd ../../OneDrive/Desktop/turb/proj-turbulence/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "bb359ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = fine.get('wind_u')\n",
    "v = fine.get('wind_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b0348692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lat in range(121)\n",
    "#     for lon in range(201):\n",
    "#         # the set of wind speeds at all time stamps at this one point\n",
    "#         wind_speeds = [np.sqrt((u[i][1][lat][lon])**2 + (v[i][1][lat][lon])**2) for i in range (744)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c96719f",
   "metadata": {},
   "source": [
    "List comprehension of wind speeds at level 225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9345852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_speeds = [[[np.sqrt((u[i][1][lat][lon])**2 + (v[i][1][lat][lon])**2) for lon in range(200)] for lat in range(120)] for i in range(744)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "cffe094a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(744, 120, 200)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wind_speeds), len(wind_speeds[0]), len(wind_speeds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "11529508",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22966fb",
   "metadata": {},
   "source": [
    "Add to coarse and fine lists in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b5af2aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_train = []\n",
    "fine_train = []\n",
    "for file in files: # loop thru files\n",
    "    if file[0]==\"w\":\n",
    "        timestamp = file[19:-4]\n",
    "        coarse_train.append(np.load(file)) # load file, save into coarse\n",
    "        fine_train.append(wind_speeds[int(timestamp)]) # load fine version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7852ca5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [185]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwind_speeds.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwind_speeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\json\\__init__.py:180\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m--> 180\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"wind_speeds.json\", \"w\") as file:\n",
    "    json.dump(wind_speeds, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbdd468",
   "metadata": {},
   "source": [
    "595 samples, # lat points, # lon points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1dc99b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(595, 15, 25)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coarse_train), len(coarse_train[0]), len(coarse_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f383e1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(595, 120, 200)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fine_train), len(fine_train[0]), len(fine_train[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c014eaae",
   "metadata": {},
   "source": [
    "## Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "fc09bd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\llaur\\OneDrive\\Desktop\\turb\\proj-turbulence\\val\n"
     ]
    }
   ],
   "source": [
    "%cd ../val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "3454e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(os.getcwd())\n",
    "coarse_val = []\n",
    "fine_val = []\n",
    "for file in files: # loop thru files\n",
    "    if file[0]==\"w\":\n",
    "        timestamp = file[19:-4]\n",
    "        coarse_val.append(np.load(file)) # load file, save into coarse\n",
    "        fine_val.append(wind_speeds[int(timestamp)]) # load fine version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4e1d8c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 15, 25)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coarse_val), len(coarse_val[0]), len(coarse_val[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "26a5e47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 120, 200)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fine_val), len(fine_val[0]), len(fine_val[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae49c8",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1d998618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\llaur\\OneDrive\\Desktop\\turb\\proj-turbulence\\test\n"
     ]
    }
   ],
   "source": [
    "%cd ../test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "82343d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(os.getcwd())\n",
    "coarse_test = []\n",
    "fine_test = []\n",
    "for file in files: # loop thru files\n",
    "    if file[0]==\"w\":\n",
    "        timestamp = file[19:-4]\n",
    "        coarse_test.append(np.load(file)) # load file, save into coarse\n",
    "        fine_test.append(wind_speeds[int(timestamp)]) # load fine version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "75d65022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 15, 25)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coarse_test), len(coarse_test[0]), len(coarse_test[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d8e4e7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 120, 200, numpy.float64)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fine_test), len(fine_test[0]), len(fine_test[0][0]), type(fine_test[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669540d0",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ed9073ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39139b5e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "96499421",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedImageDataset(Dataset):\n",
    "    def __init__(self, coarse_images, high_res_images, transform=None):\n",
    "        self.coarse_images = coarse_images\n",
    "        self.high_res_images = high_res_images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coarse_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        coarse_img = self.coarse_images[idx]\n",
    "        high_res_img = self.high_res_images[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            coarse_img = self.transform(coarse_img)\n",
    "            high_res_img = self.transform(high_res_img)\n",
    "\n",
    "        return coarse_img, high_res_img\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "train = PairedImageDataset(np.array(coarse_train), np.array(fine_train), transform=transform)\n",
    "test = PairedImageDataset(np.array(coarse_test), np.array(fine_test), transform=transform)\n",
    "val = PairedImageDataset(np.array(coarse_val), np.array(fine_val), transform=transform)\n",
    "\n",
    "# load\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "af730e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((595, 15, 25), (595, 120, 200))"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(coarse_train).shape, np.array(fine_train).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4288dc3",
   "metadata": {},
   "source": [
    "## Define CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "5f1a4c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        upscale = 8\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, upscale**2, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.depth_to_space = nn.PixelShuffle(upscale)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x.float())\n",
    "        x = self.depth_to_space(x)\n",
    "        return x\n",
    "    \n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89884687",
   "metadata": {},
   "source": [
    "## Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "1765876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "768611ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 558.052734375, Val Loss: 541.7008411092088\n",
      "Epoch 2/500, Loss: 627.2586059570312, Val Loss: 512.0131937266107\n",
      "Epoch 3/500, Loss: 421.6134033203125, Val Loss: 493.8916035939348\n",
      "Epoch 4/500, Loss: 439.005615234375, Val Loss: 494.602965867935\n",
      "Epoch 5/500, Loss: 432.7286682128906, Val Loss: 539.804745825562\n",
      "Epoch 6/500, Loss: 541.2561645507812, Val Loss: 540.9807608916528\n",
      "Epoch 7/500, Loss: 503.9337463378906, Val Loss: 528.66395821901\n",
      "Epoch 8/500, Loss: 637.3790893554688, Val Loss: 518.8255775813224\n",
      "Epoch 9/500, Loss: 553.5615844726562, Val Loss: 591.1838148970093\n",
      "Epoch 10/500, Loss: 513.9009399414062, Val Loss: 544.73562691653\n",
      "Epoch 11/500, Loss: 751.6361694335938, Val Loss: 464.0344192929624\n",
      "Epoch 12/500, Loss: 556.069091796875, Val Loss: 564.7656573373757\n",
      "Epoch 13/500, Loss: 675.9747314453125, Val Loss: 524.1569010990088\n",
      "Epoch 14/500, Loss: 399.5264892578125, Val Loss: 570.6912068938872\n",
      "Epoch 15/500, Loss: 620.0687866210938, Val Loss: 520.7096576836695\n",
      "Epoch 16/500, Loss: 453.7633056640625, Val Loss: 537.7077756374721\n",
      "Epoch 17/500, Loss: 602.469970703125, Val Loss: 445.2992334597564\n",
      "Epoch 18/500, Loss: 592.5609741210938, Val Loss: 513.0481028885506\n",
      "Epoch 19/500, Loss: 654.35009765625, Val Loss: 513.5361131478153\n",
      "Epoch 20/500, Loss: 720.0054931640625, Val Loss: 520.9848127810343\n",
      "Epoch 21/500, Loss: 650.2664184570312, Val Loss: 498.65822260244727\n",
      "Epoch 22/500, Loss: 546.9025268554688, Val Loss: 526.6661585027415\n",
      "Epoch 23/500, Loss: 586.0028076171875, Val Loss: 535.0787087631159\n",
      "Epoch 24/500, Loss: 647.640869140625, Val Loss: 502.664678612234\n",
      "Epoch 25/500, Loss: 528.8795776367188, Val Loss: 505.6158420936208\n",
      "Epoch 26/500, Loss: 557.4305419921875, Val Loss: 544.5355717882397\n",
      "Epoch 27/500, Loss: 482.343505859375, Val Loss: 552.3550786590408\n",
      "Epoch 28/500, Loss: 484.9644775390625, Val Loss: 577.88033580229\n",
      "Epoch 29/500, Loss: 432.6977844238281, Val Loss: 512.4082579241539\n",
      "Epoch 30/500, Loss: 417.0052185058594, Val Loss: 517.1493791045572\n",
      "Epoch 31/500, Loss: 475.0863342285156, Val Loss: 493.3163444157448\n",
      "Epoch 32/500, Loss: 674.5894775390625, Val Loss: 527.4152471863741\n",
      "Epoch 33/500, Loss: 439.8562927246094, Val Loss: 552.6187678909058\n",
      "Epoch 34/500, Loss: 571.1875610351562, Val Loss: 551.9115692364728\n",
      "Epoch 35/500, Loss: 566.2503051757812, Val Loss: 506.5665855070368\n",
      "Epoch 36/500, Loss: 501.39129638671875, Val Loss: 518.4759096086864\n",
      "Epoch 37/500, Loss: 627.2271118164062, Val Loss: 518.1836373855461\n",
      "Epoch 38/500, Loss: 677.2634887695312, Val Loss: 511.25373251687773\n",
      "Epoch 39/500, Loss: 531.20556640625, Val Loss: 518.422734974505\n",
      "Epoch 40/500, Loss: 521.2617797851562, Val Loss: 513.3160676064002\n",
      "Epoch 41/500, Loss: 510.7534484863281, Val Loss: 478.21482855154176\n",
      "Epoch 42/500, Loss: 713.3367309570312, Val Loss: 541.3243018302793\n",
      "Epoch 43/500, Loss: 520.8688354492188, Val Loss: 494.3615619919164\n",
      "Epoch 44/500, Loss: 439.9375915527344, Val Loss: 484.7280175259633\n",
      "Epoch 45/500, Loss: 485.7380065917969, Val Loss: 515.8219420899186\n",
      "Epoch 46/500, Loss: 456.6332092285156, Val Loss: 545.2793136317064\n",
      "Epoch 47/500, Loss: 557.539794921875, Val Loss: 515.1721333888819\n",
      "Epoch 48/500, Loss: 456.1318664550781, Val Loss: 534.0439557211539\n",
      "Epoch 49/500, Loss: 569.4877319335938, Val Loss: 510.51763186179494\n",
      "Epoch 50/500, Loss: 483.0421447753906, Val Loss: 530.3314002811325\n",
      "Epoch 51/500, Loss: 553.5859375, Val Loss: 547.905126955224\n",
      "Epoch 52/500, Loss: 618.487548828125, Val Loss: 557.2058056840513\n",
      "Epoch 53/500, Loss: 582.066650390625, Val Loss: 587.8569970260804\n",
      "Epoch 54/500, Loss: 505.1355285644531, Val Loss: 559.2485181207551\n",
      "Epoch 55/500, Loss: 478.6470642089844, Val Loss: 533.8378642323485\n",
      "Epoch 56/500, Loss: 610.7971801757812, Val Loss: 529.6194352082615\n",
      "Epoch 57/500, Loss: 542.5914306640625, Val Loss: 524.0662266530661\n",
      "Epoch 58/500, Loss: 738.2105712890625, Val Loss: 491.2725675293984\n",
      "Epoch 59/500, Loss: 546.244140625, Val Loss: 521.13937964292\n",
      "Epoch 60/500, Loss: 614.0595703125, Val Loss: 529.0507967555889\n",
      "Epoch 61/500, Loss: 513.0215454101562, Val Loss: 541.1074116073055\n",
      "Epoch 62/500, Loss: 537.889892578125, Val Loss: 492.229912828497\n",
      "Epoch 63/500, Loss: 574.4717407226562, Val Loss: 577.1906642564064\n",
      "Epoch 64/500, Loss: 550.2525024414062, Val Loss: 521.2864775217729\n",
      "Epoch 65/500, Loss: 497.4693298339844, Val Loss: 505.13184009048103\n",
      "Epoch 66/500, Loss: 560.2298583984375, Val Loss: 549.6913857682644\n",
      "Epoch 67/500, Loss: 428.2818908691406, Val Loss: 521.121986710292\n",
      "Epoch 68/500, Loss: 497.0165100097656, Val Loss: 507.63129266151145\n",
      "Epoch 69/500, Loss: 458.7666015625, Val Loss: 451.1997703482701\n",
      "Epoch 70/500, Loss: 757.4222412109375, Val Loss: 483.17754069035624\n",
      "Epoch 71/500, Loss: 671.8251342773438, Val Loss: 506.18363651718266\n",
      "Epoch 72/500, Loss: 483.4435119628906, Val Loss: 531.0495036070354\n",
      "Epoch 73/500, Loss: 478.66595458984375, Val Loss: 499.50729577028505\n",
      "Epoch 74/500, Loss: 467.0419921875, Val Loss: 523.9538651530944\n",
      "Epoch 75/500, Loss: 545.7938232421875, Val Loss: 520.8483888909308\n",
      "Epoch 76/500, Loss: 636.1945190429688, Val Loss: 507.00326449123855\n",
      "Epoch 77/500, Loss: 659.2731323242188, Val Loss: 571.108460551912\n",
      "Epoch 78/500, Loss: 649.5944213867188, Val Loss: 532.1144232974542\n",
      "Epoch 79/500, Loss: 407.9440002441406, Val Loss: 500.3359294023486\n",
      "Epoch 80/500, Loss: 632.2789916992188, Val Loss: 582.9496718651985\n",
      "Epoch 81/500, Loss: 580.215087890625, Val Loss: 524.3056230571095\n",
      "Epoch 82/500, Loss: 615.0928344726562, Val Loss: 487.7198303313181\n",
      "Epoch 83/500, Loss: 557.7963256835938, Val Loss: 576.5615347376121\n",
      "Epoch 84/500, Loss: 592.3474731445312, Val Loss: 541.9223343725088\n",
      "Epoch 85/500, Loss: 514.3767700195312, Val Loss: 507.95039453205146\n",
      "Epoch 86/500, Loss: 714.3699951171875, Val Loss: 527.0187584547737\n",
      "Epoch 87/500, Loss: 453.15631103515625, Val Loss: 512.8335271320144\n",
      "Epoch 88/500, Loss: 698.859130859375, Val Loss: 533.2654040784073\n",
      "Epoch 89/500, Loss: 619.25341796875, Val Loss: 536.9047672645331\n",
      "Epoch 90/500, Loss: 660.4653930664062, Val Loss: 518.5999149779951\n",
      "Epoch 91/500, Loss: 833.282470703125, Val Loss: 535.7735220169699\n",
      "Epoch 92/500, Loss: 516.3218383789062, Val Loss: 547.0056962892415\n",
      "Epoch 93/500, Loss: 477.7917785644531, Val Loss: 556.0402255540672\n",
      "Epoch 94/500, Loss: 577.1716918945312, Val Loss: 558.1326596320324\n",
      "Epoch 95/500, Loss: 539.27294921875, Val Loss: 495.14963474463923\n",
      "Epoch 96/500, Loss: 515.8946533203125, Val Loss: 523.0208809651874\n",
      "Epoch 97/500, Loss: 583.9615478515625, Val Loss: 523.7272212025986\n",
      "Epoch 98/500, Loss: 490.3017883300781, Val Loss: 546.7477594519557\n",
      "Epoch 99/500, Loss: 528.0009155273438, Val Loss: 572.4701658872433\n",
      "Epoch 100/500, Loss: 506.96429443359375, Val Loss: 491.3839700828088\n",
      "Epoch 101/500, Loss: 487.1950988769531, Val Loss: 479.5366151875309\n",
      "Epoch 102/500, Loss: 544.0977172851562, Val Loss: 513.3912671385787\n",
      "Epoch 103/500, Loss: 546.63232421875, Val Loss: 497.6205854654856\n",
      "Epoch 104/500, Loss: 567.399169921875, Val Loss: 546.5707861842967\n",
      "Epoch 105/500, Loss: 654.868896484375, Val Loss: 498.0161722271149\n",
      "Epoch 106/500, Loss: 532.1824951171875, Val Loss: 510.3175559696826\n",
      "Epoch 107/500, Loss: 520.7950439453125, Val Loss: 535.4964069196224\n",
      "Epoch 108/500, Loss: 653.547119140625, Val Loss: 527.3359738374487\n",
      "Epoch 109/500, Loss: 788.5558471679688, Val Loss: 478.28478280990856\n",
      "Epoch 110/500, Loss: 659.03125, Val Loss: 498.2680452820181\n",
      "Epoch 111/500, Loss: 723.9121704101562, Val Loss: 572.6036567839375\n",
      "Epoch 112/500, Loss: 570.7831420898438, Val Loss: 549.3290625925858\n",
      "Epoch 113/500, Loss: 574.2344360351562, Val Loss: 513.026957841928\n",
      "Epoch 114/500, Loss: 532.7455444335938, Val Loss: 525.7747343338668\n",
      "Epoch 115/500, Loss: 498.1662902832031, Val Loss: 522.3583184456996\n",
      "Epoch 116/500, Loss: 437.503662109375, Val Loss: 484.3622089506303\n",
      "Epoch 117/500, Loss: 534.4599609375, Val Loss: 453.24384665123944\n",
      "Epoch 118/500, Loss: 793.3165893554688, Val Loss: 555.221437519137\n",
      "Epoch 119/500, Loss: 537.2155151367188, Val Loss: 587.7022326526461\n",
      "Epoch 120/500, Loss: 558.2689208984375, Val Loss: 523.0049371805005\n",
      "Epoch 121/500, Loss: 478.472900390625, Val Loss: 510.2231604420564\n",
      "Epoch 122/500, Loss: 455.1717834472656, Val Loss: 543.1017507977015\n",
      "Epoch 123/500, Loss: 603.4317626953125, Val Loss: 495.03660153868833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/500, Loss: 633.794677734375, Val Loss: 593.3012163569251\n",
      "Epoch 125/500, Loss: 493.19232177734375, Val Loss: 536.2586574454835\n",
      "Epoch 126/500, Loss: 567.9945068359375, Val Loss: 478.9677832604891\n",
      "Epoch 127/500, Loss: 472.8819274902344, Val Loss: 564.6824434267171\n",
      "Epoch 128/500, Loss: 594.5746459960938, Val Loss: 525.9294417700177\n",
      "Epoch 129/500, Loss: 557.1177978515625, Val Loss: 534.1133834911135\n",
      "Epoch 130/500, Loss: 481.644287109375, Val Loss: 565.1030779405966\n",
      "Epoch 131/500, Loss: 646.2188720703125, Val Loss: 534.0332220561153\n",
      "Epoch 132/500, Loss: 461.5723571777344, Val Loss: 582.681277115425\n",
      "Epoch 133/500, Loss: 605.5538330078125, Val Loss: 509.3901981928063\n",
      "Epoch 134/500, Loss: 632.7379150390625, Val Loss: 557.5695569700764\n",
      "Epoch 135/500, Loss: 581.7783813476562, Val Loss: 487.58639552027546\n",
      "Epoch 136/500, Loss: 605.5850830078125, Val Loss: 504.8478000519384\n",
      "Epoch 137/500, Loss: 582.7991333007812, Val Loss: 519.5891466939571\n",
      "Epoch 138/500, Loss: 454.60235595703125, Val Loss: 495.6339360007396\n",
      "Epoch 139/500, Loss: 483.3900146484375, Val Loss: 499.9249922835687\n",
      "Epoch 140/500, Loss: 618.9451293945312, Val Loss: 536.2355284382508\n",
      "Epoch 141/500, Loss: 585.0411987304688, Val Loss: 532.7971267685652\n",
      "Epoch 142/500, Loss: 505.5641174316406, Val Loss: 585.4633987579705\n",
      "Epoch 143/500, Loss: 537.1727294921875, Val Loss: 522.6604290804822\n",
      "Epoch 144/500, Loss: 451.7274169921875, Val Loss: 537.2803938489826\n",
      "Epoch 145/500, Loss: 660.12353515625, Val Loss: 512.9638075986793\n",
      "Epoch 146/500, Loss: 575.2691650390625, Val Loss: 505.2815514914521\n",
      "Epoch 147/500, Loss: 638.4884643554688, Val Loss: 562.7210452957551\n",
      "Epoch 148/500, Loss: 680.02294921875, Val Loss: 533.9468081457776\n",
      "Epoch 149/500, Loss: 527.9583129882812, Val Loss: 491.804309999229\n",
      "Epoch 150/500, Loss: 517.4307861328125, Val Loss: 550.7685178161003\n",
      "Epoch 151/500, Loss: 618.4328002929688, Val Loss: 553.9864137505353\n",
      "Epoch 152/500, Loss: 563.3338012695312, Val Loss: 510.3278457005914\n",
      "Epoch 153/500, Loss: 652.4947509765625, Val Loss: 560.4320988144343\n",
      "Epoch 154/500, Loss: 688.4441528320312, Val Loss: 552.141150920058\n",
      "Epoch 155/500, Loss: 521.8587646484375, Val Loss: 494.8758399318772\n",
      "Epoch 156/500, Loss: 471.1131591796875, Val Loss: 519.367966828532\n",
      "Epoch 157/500, Loss: 493.5002136230469, Val Loss: 542.7369541996508\n",
      "Epoch 158/500, Loss: 693.1868896484375, Val Loss: 533.4511608831704\n",
      "Epoch 159/500, Loss: 643.8141479492188, Val Loss: 504.08611386786004\n",
      "Epoch 160/500, Loss: 554.7388305664062, Val Loss: 492.5064443796289\n",
      "Epoch 161/500, Loss: 592.480224609375, Val Loss: 547.0333401752283\n",
      "Epoch 162/500, Loss: 481.1900329589844, Val Loss: 529.251585427389\n",
      "Epoch 163/500, Loss: 495.71868896484375, Val Loss: 508.12112176101544\n",
      "Epoch 164/500, Loss: 517.1659545898438, Val Loss: 546.1244277320999\n",
      "Epoch 165/500, Loss: 561.2131958007812, Val Loss: 510.5364642390328\n",
      "Epoch 166/500, Loss: 401.4271240234375, Val Loss: 539.7642071495105\n",
      "Epoch 167/500, Loss: 706.2733154296875, Val Loss: 542.799377142953\n",
      "Epoch 168/500, Loss: 408.7958984375, Val Loss: 475.70505869452455\n",
      "Epoch 169/500, Loss: 520.949951171875, Val Loss: 521.2599147731248\n",
      "Epoch 170/500, Loss: 575.6370849609375, Val Loss: 520.2287712667747\n",
      "Epoch 171/500, Loss: 633.0382690429688, Val Loss: 533.961446026776\n",
      "Epoch 172/500, Loss: 535.8533935546875, Val Loss: 483.0354897754054\n",
      "Epoch 173/500, Loss: 502.477783203125, Val Loss: 502.6639497236392\n",
      "Epoch 174/500, Loss: 551.3975219726562, Val Loss: 543.0352110682749\n",
      "Epoch 175/500, Loss: 520.3421020507812, Val Loss: 521.2756051994593\n",
      "Epoch 176/500, Loss: 545.1239624023438, Val Loss: 540.3975253287861\n",
      "Epoch 177/500, Loss: 438.1076965332031, Val Loss: 540.8788730928355\n",
      "Epoch 178/500, Loss: 556.8738403320312, Val Loss: 496.7093486238966\n",
      "Epoch 179/500, Loss: 657.0564575195312, Val Loss: 541.1009230788403\n",
      "Epoch 180/500, Loss: 415.0320129394531, Val Loss: 535.8805823113391\n",
      "Epoch 181/500, Loss: 609.7078247070312, Val Loss: 554.5522046879202\n",
      "Epoch 182/500, Loss: 636.5401611328125, Val Loss: 552.8574931156494\n",
      "Epoch 183/500, Loss: 565.8893432617188, Val Loss: 508.97043238274716\n",
      "Epoch 184/500, Loss: 545.6989135742188, Val Loss: 516.9268512452015\n",
      "Epoch 185/500, Loss: 591.701171875, Val Loss: 511.24281315322537\n",
      "Epoch 186/500, Loss: 582.3858642578125, Val Loss: 479.82371561735727\n",
      "Epoch 187/500, Loss: 447.6374816894531, Val Loss: 522.0469858759216\n",
      "Epoch 188/500, Loss: 498.6898498535156, Val Loss: 561.516094800412\n",
      "Epoch 189/500, Loss: 582.6934204101562, Val Loss: 541.0543132413512\n",
      "Epoch 190/500, Loss: 470.9537353515625, Val Loss: 522.441175841285\n",
      "Epoch 191/500, Loss: 532.160400390625, Val Loss: 561.3906368078779\n",
      "Epoch 192/500, Loss: 663.2836303710938, Val Loss: 519.0920701094888\n",
      "Epoch 193/500, Loss: 429.40704345703125, Val Loss: 562.530592497032\n",
      "Epoch 194/500, Loss: 605.4260864257812, Val Loss: 528.5581843769802\n",
      "Epoch 195/500, Loss: 498.3752136230469, Val Loss: 519.8216791087668\n",
      "Epoch 196/500, Loss: 513.447265625, Val Loss: 564.4928313891161\n",
      "Epoch 197/500, Loss: 643.0342407226562, Val Loss: 491.2708355404372\n",
      "Epoch 198/500, Loss: 430.1186218261719, Val Loss: 452.34934850549223\n",
      "Epoch 199/500, Loss: 519.39306640625, Val Loss: 525.9598181469471\n",
      "Epoch 200/500, Loss: 453.1395568847656, Val Loss: 481.98722783334813\n",
      "Epoch 201/500, Loss: 615.08935546875, Val Loss: 540.2436013845631\n",
      "Epoch 202/500, Loss: 718.769775390625, Val Loss: 539.450466216675\n",
      "Epoch 203/500, Loss: 594.154541015625, Val Loss: 508.5291341696522\n",
      "Epoch 204/500, Loss: 554.4527587890625, Val Loss: 516.5111991996095\n",
      "Epoch 205/500, Loss: 557.7465209960938, Val Loss: 566.3983748095542\n",
      "Epoch 206/500, Loss: 617.0177612304688, Val Loss: 554.7372152729757\n",
      "Epoch 207/500, Loss: 550.1376953125, Val Loss: 494.82701998509555\n",
      "Epoch 208/500, Loss: 562.6287231445312, Val Loss: 507.99815788648493\n",
      "Epoch 209/500, Loss: 679.123779296875, Val Loss: 508.6784955738887\n",
      "Epoch 210/500, Loss: 535.441650390625, Val Loss: 554.0829461751323\n",
      "Epoch 211/500, Loss: 591.3072509765625, Val Loss: 557.16966026505\n",
      "Epoch 212/500, Loss: 592.1180419921875, Val Loss: 569.9944856101254\n",
      "Epoch 213/500, Loss: 556.8273315429688, Val Loss: 534.1182928501975\n",
      "Epoch 214/500, Loss: 549.7999877929688, Val Loss: 510.940923555431\n",
      "Epoch 215/500, Loss: 511.12921142578125, Val Loss: 485.6521840079989\n",
      "Epoch 216/500, Loss: 535.2867431640625, Val Loss: 529.4309931840495\n",
      "Epoch 217/500, Loss: 680.656005859375, Val Loss: 536.2546684025738\n",
      "Epoch 218/500, Loss: 646.9592895507812, Val Loss: 540.2601497384917\n",
      "Epoch 219/500, Loss: 621.3948974609375, Val Loss: 506.7197661441065\n",
      "Epoch 220/500, Loss: 533.7205810546875, Val Loss: 533.7630836312497\n",
      "Epoch 221/500, Loss: 749.9867553710938, Val Loss: 546.600150302024\n",
      "Epoch 222/500, Loss: 546.6776733398438, Val Loss: 510.3025480184985\n",
      "Epoch 223/500, Loss: 435.1948547363281, Val Loss: 630.6101870860527\n",
      "Epoch 224/500, Loss: 586.9088745117188, Val Loss: 529.8695229105892\n",
      "Epoch 225/500, Loss: 466.6795654296875, Val Loss: 552.7735699158395\n",
      "Epoch 226/500, Loss: 429.76422119140625, Val Loss: 538.8360836282196\n",
      "Epoch 227/500, Loss: 391.2958984375, Val Loss: 512.747572572213\n",
      "Epoch 228/500, Loss: 536.8510131835938, Val Loss: 496.21505311322124\n",
      "Epoch 229/500, Loss: 560.029541015625, Val Loss: 577.4635207731059\n",
      "Epoch 230/500, Loss: 530.9096069335938, Val Loss: 501.78292379106455\n",
      "Epoch 231/500, Loss: 473.3890075683594, Val Loss: 512.3489690465419\n",
      "Epoch 232/500, Loss: 572.3021240234375, Val Loss: 538.3560499462071\n",
      "Epoch 233/500, Loss: 674.4178466796875, Val Loss: 492.80830781537014\n",
      "Epoch 234/500, Loss: 635.5848388671875, Val Loss: 585.9308277515202\n",
      "Epoch 235/500, Loss: 693.639404296875, Val Loss: 524.0670389879718\n",
      "Epoch 236/500, Loss: 548.2308959960938, Val Loss: 527.15405576952\n",
      "Epoch 237/500, Loss: 507.7605285644531, Val Loss: 524.0869291976329\n",
      "Epoch 238/500, Loss: 642.2362670898438, Val Loss: 514.3600289380213\n",
      "Epoch 239/500, Loss: 662.337646484375, Val Loss: 594.2555764501213\n",
      "Epoch 240/500, Loss: 558.5728149414062, Val Loss: 527.439620741473\n",
      "Epoch 241/500, Loss: 529.11962890625, Val Loss: 584.0211621657236\n",
      "Epoch 242/500, Loss: 555.5419921875, Val Loss: 479.24519486487685\n",
      "Epoch 243/500, Loss: 496.5133361816406, Val Loss: 483.89768342664615\n",
      "Epoch 244/500, Loss: 530.0394897460938, Val Loss: 545.1875491265683\n",
      "Epoch 245/500, Loss: 535.0106201171875, Val Loss: 467.5382699466123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246/500, Loss: 540.4950561523438, Val Loss: 498.58976988310894\n",
      "Epoch 247/500, Loss: 495.836669921875, Val Loss: 525.6029539728906\n",
      "Epoch 248/500, Loss: 539.7841796875, Val Loss: 522.618744097931\n",
      "Epoch 249/500, Loss: 553.9036865234375, Val Loss: 450.1742231922154\n",
      "Epoch 250/500, Loss: 622.9188232421875, Val Loss: 521.4479406281804\n",
      "Epoch 251/500, Loss: 589.5850830078125, Val Loss: 542.8212893421439\n",
      "Epoch 252/500, Loss: 557.254150390625, Val Loss: 523.378388186698\n",
      "Epoch 253/500, Loss: 593.3197021484375, Val Loss: 568.6989319143788\n",
      "Epoch 254/500, Loss: 512.8455200195312, Val Loss: 521.2565537199077\n",
      "Epoch 255/500, Loss: 615.5069580078125, Val Loss: 564.1797124876693\n",
      "Epoch 256/500, Loss: 677.3341674804688, Val Loss: 527.315999658588\n",
      "Epoch 257/500, Loss: 438.55548095703125, Val Loss: 582.6657138736155\n",
      "Epoch 258/500, Loss: 529.1700439453125, Val Loss: 519.0679714355666\n",
      "Epoch 259/500, Loss: 594.4030151367188, Val Loss: 520.0934080864021\n",
      "Epoch 260/500, Loss: 507.5894775390625, Val Loss: 589.5928927047069\n",
      "Epoch 261/500, Loss: 475.7068786621094, Val Loss: 522.6195828125333\n",
      "Epoch 262/500, Loss: 556.4764404296875, Val Loss: 474.72760688688487\n",
      "Epoch 263/500, Loss: 541.3284301757812, Val Loss: 509.2980518906367\n",
      "Epoch 264/500, Loss: 541.7237548828125, Val Loss: 515.0663826010147\n",
      "Epoch 265/500, Loss: 490.6077880859375, Val Loss: 525.7249643254777\n",
      "Epoch 266/500, Loss: 482.46917724609375, Val Loss: 480.29158782170526\n",
      "Epoch 267/500, Loss: 417.8450012207031, Val Loss: 497.3326016417012\n",
      "Epoch 268/500, Loss: 541.957275390625, Val Loss: 541.5574313286654\n",
      "Epoch 269/500, Loss: 603.2679443359375, Val Loss: 476.2298969038625\n",
      "Epoch 270/500, Loss: 538.905029296875, Val Loss: 478.020490796922\n",
      "Epoch 271/500, Loss: 657.7735595703125, Val Loss: 543.1143218673042\n",
      "Epoch 272/500, Loss: 647.9860229492188, Val Loss: 505.1200566746409\n",
      "Epoch 273/500, Loss: 566.7640380859375, Val Loss: 566.4213476080183\n",
      "Epoch 274/500, Loss: 411.22869873046875, Val Loss: 518.8617282579503\n",
      "Epoch 275/500, Loss: 523.394287109375, Val Loss: 552.932775665938\n",
      "Epoch 276/500, Loss: 543.5731201171875, Val Loss: 531.3160842853282\n",
      "Epoch 277/500, Loss: 481.4801330566406, Val Loss: 550.4209464707112\n",
      "Epoch 278/500, Loss: 530.0765380859375, Val Loss: 525.4525596417427\n",
      "Epoch 279/500, Loss: 663.2354125976562, Val Loss: 529.2118347004335\n",
      "Epoch 280/500, Loss: 604.6182250976562, Val Loss: 543.1279592977414\n",
      "Epoch 281/500, Loss: 480.7015686035156, Val Loss: 581.5387619744032\n",
      "Epoch 282/500, Loss: 450.4266662597656, Val Loss: 540.4331274786146\n",
      "Epoch 283/500, Loss: 569.3794555664062, Val Loss: 513.5209830808943\n",
      "Epoch 284/500, Loss: 525.2909545898438, Val Loss: 526.0416395511307\n",
      "Epoch 285/500, Loss: 673.518310546875, Val Loss: 474.89129484474046\n",
      "Epoch 286/500, Loss: 466.0511169433594, Val Loss: 511.65227211367625\n",
      "Epoch 287/500, Loss: 605.0487670898438, Val Loss: 481.6134892135336\n",
      "Epoch 288/500, Loss: 487.2876281738281, Val Loss: 486.92593739236463\n",
      "Epoch 289/500, Loss: 509.84344482421875, Val Loss: 524.8195961551032\n",
      "Epoch 290/500, Loss: 775.7635498046875, Val Loss: 566.6897737426611\n",
      "Epoch 291/500, Loss: 429.8206787109375, Val Loss: 579.712484190102\n",
      "Epoch 292/500, Loss: 613.0441284179688, Val Loss: 506.2629866220355\n",
      "Epoch 293/500, Loss: 545.4945678710938, Val Loss: 476.71169081887604\n",
      "Epoch 294/500, Loss: 603.9464111328125, Val Loss: 517.4249054368028\n",
      "Epoch 295/500, Loss: 538.8096313476562, Val Loss: 551.5120157727204\n",
      "Epoch 296/500, Loss: 480.6378173828125, Val Loss: 489.49127555167985\n",
      "Epoch 297/500, Loss: 615.9730834960938, Val Loss: 498.3550730395668\n",
      "Epoch 298/500, Loss: 458.73614501953125, Val Loss: 538.346321586213\n",
      "Epoch 299/500, Loss: 632.0711669921875, Val Loss: 504.9630389923869\n",
      "Epoch 300/500, Loss: 538.128662109375, Val Loss: 473.3841430849758\n",
      "Epoch 301/500, Loss: 667.37646484375, Val Loss: 509.8012259958594\n",
      "Epoch 302/500, Loss: 494.81524658203125, Val Loss: 549.3438061648513\n",
      "Epoch 303/500, Loss: 510.48028564453125, Val Loss: 538.3367236087945\n",
      "Epoch 304/500, Loss: 569.8914184570312, Val Loss: 534.2080632080388\n",
      "Epoch 305/500, Loss: 620.4727783203125, Val Loss: 538.7140714202372\n",
      "Epoch 306/500, Loss: 523.4530639648438, Val Loss: 526.1159875050347\n",
      "Epoch 307/500, Loss: 394.8350830078125, Val Loss: 583.8437711598053\n",
      "Epoch 308/500, Loss: 535.257568359375, Val Loss: 538.7759272923437\n",
      "Epoch 309/500, Loss: 499.6919250488281, Val Loss: 500.7184936830304\n",
      "Epoch 310/500, Loss: 666.8357543945312, Val Loss: 511.9048685607109\n",
      "Epoch 311/500, Loss: 483.96087646484375, Val Loss: 528.8838727561775\n",
      "Epoch 312/500, Loss: 573.122802734375, Val Loss: 507.1109092403016\n",
      "Epoch 313/500, Loss: 601.2354736328125, Val Loss: 535.8699029685821\n",
      "Epoch 314/500, Loss: 466.9232482910156, Val Loss: 592.7729317149635\n",
      "Epoch 315/500, Loss: 543.9080810546875, Val Loss: 454.95004636138844\n",
      "Epoch 316/500, Loss: 554.5775756835938, Val Loss: 573.0544474716812\n",
      "Epoch 317/500, Loss: 581.7466430664062, Val Loss: 564.2443956477415\n",
      "Epoch 318/500, Loss: 527.4638671875, Val Loss: 518.3527382057752\n",
      "Epoch 319/500, Loss: 570.67578125, Val Loss: 469.71580400648656\n",
      "Epoch 320/500, Loss: 544.8504028320312, Val Loss: 566.4978541699568\n",
      "Epoch 321/500, Loss: 505.6476745605469, Val Loss: 505.41979862376627\n",
      "Epoch 322/500, Loss: 643.4569091796875, Val Loss: 548.6890435283823\n",
      "Epoch 323/500, Loss: 615.0430908203125, Val Loss: 469.1887722066302\n",
      "Epoch 324/500, Loss: 523.4291381835938, Val Loss: 529.0634095942358\n",
      "Epoch 325/500, Loss: 592.2639770507812, Val Loss: 557.4204261581541\n",
      "Epoch 326/500, Loss: 624.409423828125, Val Loss: 551.1404987626929\n",
      "Epoch 327/500, Loss: 526.8218383789062, Val Loss: 515.8904618148417\n",
      "Epoch 328/500, Loss: 563.8329467773438, Val Loss: 482.67554779314804\n",
      "Epoch 329/500, Loss: 480.6930236816406, Val Loss: 569.1000651100211\n",
      "Epoch 330/500, Loss: 688.8353881835938, Val Loss: 520.0269483651523\n",
      "Epoch 331/500, Loss: 495.6214904785156, Val Loss: 551.9090532720379\n",
      "Epoch 332/500, Loss: 501.43695068359375, Val Loss: 530.0271758051309\n",
      "Epoch 333/500, Loss: 575.4232788085938, Val Loss: 487.31429154549454\n",
      "Epoch 334/500, Loss: 599.8239135742188, Val Loss: 543.0925006147507\n",
      "Epoch 335/500, Loss: 607.4921875, Val Loss: 599.7412553495735\n",
      "Epoch 336/500, Loss: 620.4031982421875, Val Loss: 472.56281095885174\n",
      "Epoch 337/500, Loss: 554.7927856445312, Val Loss: 534.5814503198047\n",
      "Epoch 338/500, Loss: 527.3466796875, Val Loss: 555.2215898690318\n",
      "Epoch 339/500, Loss: 593.7426147460938, Val Loss: 514.70574424052\n",
      "Epoch 340/500, Loss: 458.95208740234375, Val Loss: 549.417211283687\n",
      "Epoch 341/500, Loss: 559.1416625976562, Val Loss: 573.8474956072926\n",
      "Epoch 342/500, Loss: 513.74609375, Val Loss: 475.9949942501237\n",
      "Epoch 343/500, Loss: 712.3019409179688, Val Loss: 521.2958095174264\n",
      "Epoch 344/500, Loss: 602.66943359375, Val Loss: 467.57752440021466\n",
      "Epoch 345/500, Loss: 739.908447265625, Val Loss: 585.2647022588556\n",
      "Epoch 346/500, Loss: 763.1522827148438, Val Loss: 490.17541162073957\n",
      "Epoch 347/500, Loss: 520.4657592773438, Val Loss: 521.5479908806228\n",
      "Epoch 348/500, Loss: 609.027587890625, Val Loss: 558.1727113262419\n",
      "Epoch 349/500, Loss: 541.7677001953125, Val Loss: 561.0946752297593\n",
      "Epoch 350/500, Loss: 587.902099609375, Val Loss: 527.1736170819328\n",
      "Epoch 351/500, Loss: 483.9000244140625, Val Loss: 470.179701735109\n",
      "Epoch 352/500, Loss: 648.3505859375, Val Loss: 512.0143525885837\n",
      "Epoch 353/500, Loss: 498.144775390625, Val Loss: 515.6784541247276\n",
      "Epoch 354/500, Loss: 665.1847534179688, Val Loss: 493.5070786852275\n",
      "Epoch 355/500, Loss: 471.7330017089844, Val Loss: 541.6469116962808\n",
      "Epoch 356/500, Loss: 599.5726928710938, Val Loss: 478.8994061642667\n",
      "Epoch 357/500, Loss: 446.87176513671875, Val Loss: 516.6861923224484\n",
      "Epoch 358/500, Loss: 755.6982421875, Val Loss: 524.8248227425391\n",
      "Epoch 359/500, Loss: 692.0662231445312, Val Loss: 536.3190485986171\n",
      "Epoch 360/500, Loss: 485.8379211425781, Val Loss: 513.4132370418214\n",
      "Epoch 361/500, Loss: 547.0240478515625, Val Loss: 542.8717206031894\n",
      "Epoch 362/500, Loss: 618.281005859375, Val Loss: 540.7765983536227\n",
      "Epoch 363/500, Loss: 522.6781616210938, Val Loss: 535.9937735563249\n",
      "Epoch 364/500, Loss: 493.6250305175781, Val Loss: 552.5073691534722\n",
      "Epoch 365/500, Loss: 524.3370971679688, Val Loss: 476.2035030188527\n",
      "Epoch 366/500, Loss: 610.1461181640625, Val Loss: 553.5814561518647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 367/500, Loss: 497.1742248535156, Val Loss: 569.8825847032133\n",
      "Epoch 368/500, Loss: 416.5218200683594, Val Loss: 584.6950686790434\n",
      "Epoch 369/500, Loss: 547.783935546875, Val Loss: 559.5193517187222\n",
      "Epoch 370/500, Loss: 446.0861511230469, Val Loss: 504.7225798840112\n",
      "Epoch 371/500, Loss: 511.6707458496094, Val Loss: 524.7864585085191\n",
      "Epoch 372/500, Loss: 475.0406799316406, Val Loss: 439.6550724154088\n",
      "Epoch 373/500, Loss: 461.11651611328125, Val Loss: 532.177598407701\n",
      "Epoch 374/500, Loss: 546.159912109375, Val Loss: 535.2571436065394\n",
      "Epoch 375/500, Loss: 555.4522094726562, Val Loss: 517.593743522084\n",
      "Epoch 376/500, Loss: 460.5572509765625, Val Loss: 476.680900922825\n",
      "Epoch 377/500, Loss: 519.371826171875, Val Loss: 545.7331525834535\n",
      "Epoch 378/500, Loss: 624.8457641601562, Val Loss: 535.0205027699673\n",
      "Epoch 379/500, Loss: 580.8557739257812, Val Loss: 524.201445231544\n",
      "Epoch 380/500, Loss: 425.40008544921875, Val Loss: 465.29480759147793\n",
      "Epoch 381/500, Loss: 588.3653564453125, Val Loss: 563.9598114243096\n",
      "Epoch 382/500, Loss: 557.6141967773438, Val Loss: 523.9997700911558\n",
      "Epoch 383/500, Loss: 442.708251953125, Val Loss: 470.9991604495487\n",
      "Epoch 384/500, Loss: 451.3138122558594, Val Loss: 577.5842424292061\n",
      "Epoch 385/500, Loss: 453.6838073730469, Val Loss: 526.6394143234022\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [250]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.NAdam(cnn.parameters(), lr=0.00000000000000000000000001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "#         inputs = inputs.squeeze()\n",
    "        targets = targets.squeeze() \n",
    "#         print(\"shapes::::\", inputs.shape, targets.shape)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, targets.squeeze().float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, targets.squeeze())\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Val Loss: {val_loss.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
